{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results, plot_curves\n",
    "\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    BaseCallback,\n",
    "    EvalCallback, \n",
    "    CheckpointCallback, \n",
    "    StopTrainingOnMaxEpisodes\n",
    ")\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "from nn.ppo_nn import CustomActorCriticPolicy\n",
    "from gym_env import RBCEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "# log_dir = \"/root/rbc/train_log\"\n",
    "# model_dir = '/root/rbc/models/self_play_models'\n",
    "# tensorboard_dir = os.path.join(log_dir, \"self_play_model_tensorboard\")\n",
    "# checkpoint_dir = '/root/rbc/checkpoints'\n",
    "\n",
    "log_dir = \"train_log\"\n",
    "model_dir = 'models/self_play_models'\n",
    "tensorboard_dir = os.path.join(log_dir, \"self_play_model_tensorboard\")\n",
    "checkpoint_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(model_dir, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = RBCEnv(model_dir=model_dir)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "def callback_on_new_best(): \n",
    "    print('NEW BEST MODEL SAVED')\n",
    "\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "    def __init__(self, pbar, max_episodes, verbose = 0):\n",
    "        super(ProgressBarCallback, self).__init__()\n",
    "        self._pbar = pbar\n",
    "        self.max_episodes = max_episodes\n",
    "        self._total_max_episodes = max_episodes\n",
    "        self.n_episodes = 0\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # At start set total max according to number of envirnments\n",
    "        self._total_max_episodes = self.max_episodes * self.training_env.num_envs\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        done_array = np.array(self.locals.get(\"done\") if self.locals.get(\"done\") is not None else self.locals.get(\"dones\"))\n",
    "        self.n_episodes += np.sum(done_array).item()\n",
    "        self._pbar.n = self.n_episodes\n",
    "        self._pbar.update(0)\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_max_episodes): # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_max_episodes = total_max_episodes\n",
    "        \n",
    "    def __enter__(self): # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_max_episodes)\n",
    "            \n",
    "        return ProgressBarCallback(self.pbar, self.total_max_episodes)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
    "        self.pbar.n = self.total_max_episodes\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "NEW BEST MODEL SAVED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d465c4c903754b4c9a00dd6e7d587823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to train_log\\self_play_model_tensorboard\\self_play_model_1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aniss\\OneDrive\\Dokumente\\Uni\\Semester4\\RBC\\RBC-Agent\\train\\self_play.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniss/OneDrive/Dokumente/Uni/Semester4/RBC/RBC-Agent/train/self_play.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m callback_max_episodes \u001b[39m=\u001b[39m StopTrainingOnMaxEpisodes(max_episodes\u001b[39m=\u001b[39mtotal_episodes, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniss/OneDrive/Dokumente/Uni/Semester4/RBC/RBC-Agent/train/self_play.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mwith\u001b[39;00m ProgressBarManager(total_episodes) \u001b[39mas\u001b[39;00m tqdm_callback: \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aniss/OneDrive/Dokumente/Uni/Semester4/RBC/RBC-Agent/train/self_play.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(\u001b[39mint\u001b[39;49m(\u001b[39m1e10\u001b[39;49m), callback\u001b[39m=\u001b[39;49m [callback_max_episodes, eval_callback, tqdm_callback], tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mself_play_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniss/OneDrive/Dokumente/Uni/Semester4/RBC/RBC-Agent/train/self_play.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_dir, \u001b[39m\"\u001b[39m\u001b[39mfinal_self_play_model\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aniss/OneDrive/Dokumente/Uni/Semester4/RBC/RBC-Agent/train/self_play.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m plot_results([log_dir],\u001b[39mint\u001b[39m(\u001b[39m1e10\u001b[39m), results_plotter\u001b[39m.\u001b[39mX_EPISODES, \u001b[39m\"\u001b[39m\u001b[39mPPO RBC\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:299\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    287\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    288\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    300\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    301\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    302\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    303\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    304\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    305\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    306\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    307\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    308\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    309\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:233\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    229\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    231\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 233\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    235\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:174\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    173\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 174\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    178\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\aniss\\OneDrive\\Dokumente\\Uni\\Semester4\\RBC\\RBC-Agent\\gym_env\\RBC_env.py:306\u001b[0m, in \u001b[0;36mRBCEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_learning_player \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mWHITE: \n\u001b[1;32m--> 306\u001b[0m         obs, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay_next_step(action, chess\u001b[39m.\u001b[39;49mWHITE, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwhite_sense_history, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblack_sense_history, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    308\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_learning_player \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mBLACK: \n\u001b[0;32m    309\u001b[0m         obs, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplay_next_step(action, chess\u001b[39m.\u001b[39mBLACK, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblack_sense_history, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhite_sense_history, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aniss\\OneDrive\\Dokumente\\Uni\\Semester4\\RBC\\RBC-Agent\\gym_env\\RBC_env.py:280\u001b[0m, in \u001b[0;36mRBCEnv.play_next_step\u001b[1;34m(self, action, player_color, player_sense_history, enemy_sense_history, flip)\u001b[0m\n\u001b[0;32m    277\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    278\u001b[0m \u001b[39m# print('Starting turn of ',self.game.turn)\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39m# print('Sense action ', sense_action)\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay_turn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplayers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mturn], [sense_action], end_turn_last\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    281\u001b[0m \u001b[39m# print('Board after move ', '\\n', self.board_white_obs if player_color == chess.WHITE else self.board_black_obs)\u001b[39;00m\n\u001b[0;32m    282\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_reward(player_color)\n",
      "File \u001b[1;32mc:\\Users\\aniss\\OneDrive\\Dokumente\\Uni\\Semester4\\RBC\\RBC-Agent\\gym_env\\RBC_env.py:215\u001b[0m, in \u001b[0;36mRBCEnv.play_turn\u001b[1;34m(self, game, player, sense_action, end_turn_last)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard_after_sense \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard_white_obs\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_learning_player \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mWHITE \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard_black_obs\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    213\u001b[0m \u001b[39m# print('board after sense ', '\\n',  self.board_after_sense)\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay_move(game, player, move_actions, end_turn_last)\n",
      "File \u001b[1;32mc:\\Users\\aniss\\OneDrive\\Dokumente\\Uni\\Semester4\\RBC\\RBC-Agent\\gym_env\\RBC_env.py:157\u001b[0m, in \u001b[0;36mRBCEnv.play_move\u001b[1;34m(self, game, player, move_actions, end_turn_last)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay_move\u001b[39m(\u001b[39mself\u001b[39m, game: Game, player: Player, move_actions: List[chess\u001b[39m.\u001b[39mMove], end_turn_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    141\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Runs the move phase for `player` in `game`. Does the following sequentially:\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m    :param end_turn_last: Flag indicating whether to call :meth:`Game.end_turn` before or after :meth:`Player.handle_move_result`\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     move \u001b[39m=\u001b[39m player\u001b[39m.\u001b[39;49mchoose_move(move_actions, game\u001b[39m.\u001b[39;49mget_seconds_left())\n\u001b[0;32m    158\u001b[0m     requested_move, taken_move, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_enemy_capture_square \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39mmove(move)\n\u001b[0;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m taken_move \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aniss\\OneDrive\\Dokumente\\Uni\\Semester4\\RBC\\RBC-Agent\\myBot\\SP_Sensing_WSTCKF.py:105\u001b[0m, in \u001b[0;36mselfPlaySensingWSTCKF.choose_move\u001b[1;34m(self, move_actions, seconds_left)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mturn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolor\n\u001b[0;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard\u001b[39m.\u001b[39mclear_stack()\n\u001b[1;32m--> 105\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mplay(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboard, chess\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mLimit(time\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m    107\u001b[0m \u001b[39m# # Assign the postion in Stockfish based on FEN\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m# board_fen = self.board.fen()\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m# stockfish.set_fen_position(board_fen)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[39m# print('move ', result.move)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mmove\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\site-packages\\chess\\engine.py:2759\u001b[0m, in \u001b[0;36mSimpleEngine.play\u001b[1;34m(self, board, limit, game, info, ponder, draw_offered, root_moves, options)\u001b[0m\n\u001b[0;32m   2755\u001b[0m     coro \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mwait_for(\n\u001b[0;32m   2756\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mplay(board, limit, game\u001b[39m=\u001b[39mgame, info\u001b[39m=\u001b[39minfo, ponder\u001b[39m=\u001b[39mponder, draw_offered\u001b[39m=\u001b[39mdraw_offered, root_moves\u001b[39m=\u001b[39mroot_moves, options\u001b[39m=\u001b[39moptions),\n\u001b[0;32m   2757\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_for(limit))\n\u001b[0;32m   2758\u001b[0m     future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mloop)\n\u001b[1;32m-> 2759\u001b[0m \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\aniss\\anaconda3\\envs\\rbc\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # n_env = 1\n",
    "    total_episodes = 10000\n",
    "    # env = SubprocVecEnv([make_env(model_dir, i, 0) for i in range(n_env)])\n",
    "    env = RBCEnv(model_dir=model_dir)\n",
    "    env = Monitor(env, log_dir)\n",
    "    eval_env = RBCEnv(model_dir=model_dir)\n",
    "    eval_env = Monitor(eval_env, log_dir)\n",
    "\n",
    "    # model = PPO.load(os.path.join(model_dir, \"best_model_gamma05\"))\n",
    "    model = PPO(CustomActorCriticPolicy, env, verbose = 1, tensorboard_log=tensorboard_dir)\n",
    "    model.save(os.path.join(model_dir, \"self_play_best_model\")) \n",
    "\n",
    "    # checkpoint_callback = CheckpointCallback(\n",
    "    # save_freq=10,\n",
    "    # save_path=checkpoint_dir,\n",
    "    # name_prefix=\"rl_model\"\n",
    "    # ) \n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env, \n",
    "        best_model_save_path=model_dir, \n",
    "        callback_on_new_best=callback_on_new_best(),\n",
    "        log_path=log_dir, \n",
    "        eval_freq= 5000,\n",
    "        n_eval_episodes=5, \n",
    "        deterministic= True, \n",
    "        render=False)\n",
    "\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=total_episodes, verbose=1)\n",
    "\n",
    "    with ProgressBarManager(total_episodes) as tqdm_callback: \n",
    "        model.learn(int(1e10), callback= [callback_max_episodes, eval_callback, tqdm_callback], tb_log_name=\"self_play_model\")\n",
    "    model.save(os.path.join(model_dir, \"final_self_play_model\"))\n",
    "    plot_results([log_dir],int(1e10), results_plotter.X_EPISODES, \"PPO RBC\")\n",
    "    # plot_curves([log_dir], results_plotter.X_EPISODES, \"PPO RBC\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rbc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334571f43833167cff2a32fc3fb1a5df84c2106c1727de884da5357de05be83f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
